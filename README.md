# Test_MSU
В данном задании была произведена бинарная классификация деталей. Алгоритм следующий: при помощи ImageDataGenerator выгрузил все данные, произвел все возможно допустимые аугментации, выгрузка всех изображений происходит по одному, так модель вменяемо обучается на маленьком датасете. Чуть меньше четверти данных закинул в валидационный датасет, при помощи seed=12345 запретил им повторяться. Для архитектуры сети выбрал простую сверточную, состоящую из 3 подряд сверток 3 на 3 с функцией активации ReLu, после которых производился пулинг по максимальному значению фильтра 2 на 2 (для упрощения вычислений), после этой комбинации идет сглаживающий слой, затем еще один полносвязный на 64 нейрона также с ReLu и дропаут, чтобы модель не переобучилась в случае чего. На выходе стоит один нейрон с сигмойдой на выходе, что является тпипчным для таких задач. Метрика-accuracy, оптимизатор-Адам (сгд чуть хуже показал себя), лосс-бинарная кросс-энтропия, опять же классика для такого рода задач. С эпохами чуть сложнее, эмпирически вывел, что при 20 accuracy и на обучающей и на валидационной становится равно 1 (в данном примере модель уже переобучилась на 1 эпоху, но вцелом не критичино).  В .ipynb решение с полученными результатами, в .py скрипт и в .h5 веса нейронной сети. Спасибо за возможность решить такое тестовое задание, было прикольно.  
